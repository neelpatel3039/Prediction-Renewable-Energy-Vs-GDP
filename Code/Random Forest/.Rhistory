# Grouping transactions by Customer_ID and summarising the total number of transactions and total amount spent
customer_summary <- transactions_data %>% group_by(Customer_ID) %>%
summarise(total_no_of_transactions = n(), total_amount_spent = sum(Total_Amount, na.rm = TRUE))
# Display the top 10 rows of the summary
head(customer_summary, 10)
# Question 5 answer:----
# Filtering premium customers with total amount spent >= 10000 and Selecting specific columns and merging with customers data
premium_customers <- customer_summary %>% filter(total_amount_spent >= 10000) %>%
select(Customer_ID, "Total Transaction" = total_amount_spent) %>% left_join(customers_data, by = "Customer_ID")
bar_graph_1 = transactions_data %>% select(Transaction_ID, Total_Amount, Product_ID) %>% right_join(products_data, by = "Product_ID") %>% select(Transaction_ID, Total_Amount, Product_ID, Category) %>% group_by(Category) %>% summarise(Total_Transaction_Count = n())
ggplot(bar_graph_1, aes(x = Category, y = Total_Transaction_Count)) + geom_bar(stat = "identity") + xlab("Product Category") + ylab("Number of Transaction") + ggtitle("Transactions by Product Category")
# Adding a column for Discounted_Amount with the condition (mentioned in the assignment)
transactions_data <- transactions_data %>% mutate(Discounted_Amount = if_else(Total_Amount > 100, Total_Amount * 0.9, Total_Amount))
View(transactions_data)
# Summarising transactions by Location
location_summary = full_join(customers_data, transactions_data, by = join_by(Customer_ID)) %>% group_by(Location) %>% summarise(Number_of_Transactions = n(), Total_Amount_Spent = sum(Total_Amount, na.rm = TRUE)) %>% arrange(desc(Number_of_Transactions))
location_summary
sum(location_summary$Number_of_Transactions)
# Filtering frequent customers based on transaction count condition in any month
frequent_customers_any <- transactions_2023 %>% group_by(Customer_ID, Month) %>% summarise(Number_of_Transactions = n()) %>% arrange(Customer_ID, Month, Number_of_Transactions) %>% filter(any(Number_of_Transactions >= 3)) %>% distinct(Customer_ID)
frequent_customers_any
frequent_customers_any_details
# Summarising transactions by Location
location_summary = right_join(customers_data, transactions_data, by = join_by(Customer_ID)) %>% group_by(Location) %>% summarise(Number_of_Transactions = n(), Total_Amount_Spent = sum(Total_Amount, na.rm = TRUE)) %>% arrange(desc(Number_of_Transactions))
location_summary
sum(location_summary$Number_of_Transactions)
ggplot(transactions_electronics, aes(x = Quantity, y = Total_Amount, color = Store_Location)) +
geom_point() +
stat_smooth(method = "lm", se = FALSE) +
labs(title = "Electronics Sales", x = "Quantity Sold", y = "Total Sales Amount", subtitle = "Quantity Sold vs. Total Sales Amount - Color Coded by Store Location")
transactions_electronics
library(tidyverse)
diamonds <- read_csv("diamonds.csv")
setwd("C:/Users/neelp/Downloads")
diamonds <- read_csv("diamonds.csv")
diamonds <- diamonds %>% select(-1)
# Create a scatter plot matrix of all numerical variables
numeric_vars <- diamonds %>% select(where(is.numeric))
scatter_plot_matrix <- ggpairs(numeric_vars)
library(GGally)
install.packages("GGally")
scatter_plot_matrix <- ggpairs(numeric_vars)
library(GGally)
scatter_plot_matrix <- ggpairs(numeric_vars)
# Display the scatter plot matrix
print(scatter_plot_matrix)
# Display the scatter plot matrix
print(scatter_plot_matrix)
View(diamonds)
library(tidyverse)
library(GGally)
diamonds <- read_csv("diamonds.csv")
diamonds <- diamonds %>% select(-1)
# Create a scatter plot matrix of all numerical variables
numeric_vars <- diamonds %>% select(where(is.numeric))
View(numeric_vars)
View(diamonds)
scatter_plot_matrix <- ggpairs(numeric_vars)
View(scatter_plot_matrix)
# Display the scatter plot matrix
print(scatter_plot_matrix)
setwd("C:/School/3 - Master of Science in Business Analytics and AI/1 - Fall Term (Semester 1)/1 - MBAI 5100 - Mondays - Business Analytics/Project/Prediction-Renewable-Energy-Vs-GDP/Code/Future_Scope_Neel/Random Forest")
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(readr)
library(randomForest)
library(ggplot2)
data <- read.csv("master_data_with_gdp_india.csv")
# Select relevant columns
data <- data %>%
select(Entity, Year, GDP, everything())
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, importance = TRUE)
# View the model summary
print(rf_model)
source("C:/School/3 - Master of Science in Business Analytics and AI/1 - Fall Term (Semester 1)/1 - MBAI 5100 - Mondays - Business Analytics/Project/Prediction-Renewable-Energy-Vs-GDP/Code/Future_Scope_Neel/Random Forest/randomForest.R")
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, importance = TRUE, keep.inbag = TRUE)
# View the model summary
print(rf_model)
# Extract OOB error rates
oob_error_data <- data.frame(
Trees = 1:rf_model$ntree,
OOBError = rf_model$err.rate[, "OOB"]
)
rf_model$err.rate
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, importance = TRUE)
# View the model summary
print(rf_model)
# Extract OOB error rates (MSE in this case)
oob_error_data <- data.frame(
Trees = 1:rf_model$ntree,
OOBError = rf_model$mse
)
# Plot the OOB error rate
oob_error_plot <- ggplot(oob_error_data, aes(x = Trees, y = OOBError)) +
geom_line(color = "blue") +
labs(title = "OOB Error Rate (MSE) vs. Number of Trees",
x = "Number of Trees",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, importance = TRUE)
# View the model summary
print(rf_model)
# Extract OOB error rates (MSE in this case)
oob_error_data <- data.frame(
Trees = 1:rf_model$ntree,
OOBError = rf_model$mse
)
# Plot the OOB error rate
oob_error_plot <- ggplot(oob_error_data, aes(x = Trees, y = OOBError)) +
geom_line(color = "blue") +
labs(title = "OOB Error Rate (MSE) vs. Number of Trees",
x = "Number of Trees",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Make predictions on the test data
test_predictions <- predict(rf_model, test_data)
# Create a data frame with actual and predicted values
results <- data.frame(
Actual = test_data$GDP,
Predicted = test_predictions
)
# Plot the actual values vs. predicted values
actual_vs_predicted_plot <- ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Actual vs. Predicted Values",
x = "Actual GDP",
y = "Predicted GDP") +
theme_minimal()
# Display the plot
print(actual_vs_predicted_plot)
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define a range of mtry values to test
mtry_values <- seq(1, ncol(train_data) - 1, by = 1)
# Store the OOB error rates for each mtry value
oob_errors <- data.frame(
mtry = integer(),
OOBError = numeric()
)
# Loop over different mtry values
for (mtry in mtry_values) {
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = mtry, importance = TRUE)
# Store the mtry value and corresponding OOB error
oob_errors <- rbind(oob_errors, data.frame(mtry = mtry, OOBError = tail(rf_model$mse, 1)))
}
# Plot the OOB error rate for different mtry values
oob_error_plot <- ggplot(oob_errors, aes(x = mtry, y = OOBError)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "OOB Error Rate vs. Number of Variables Tried at Each Split (mtry)",
x = "Number of Variables (mtry)",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Print all the OOB values
print(oob_errors)
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define a range of mtry values to test
mtry_values <- seq(1, ncol(train_data) - 1, by = 1)
# Store the OOB error rates for each mtry value
oob_errors <- data.frame(
mtry = integer(),
OOBError = numeric()
)
# Loop over different mtry values
for (mtry in mtry_values) {
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = mtry, importance = TRUE)
# Store the mtry value and corresponding OOB error
oob_errors <- rbind(oob_errors, data.frame(mtry = mtry, OOBError = tail(rf_model$mse, 1)))
}
# Plot the OOB error rate for different mtry values
oob_error_plot <- ggplot(oob_errors, aes(x = mtry, y = OOBError)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "OOB Error Rate vs. Number of Variables Tried at Each Split (mtry)",
x = "Number of Variables (mtry)",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Print all the OOB values
print(oob_errors)
# Identify the mtry value with the lowest OOB error rate
best_mtry <- oob_errors$mtry[which.min(oob_errors$OOBError)]
cat("Best mtry value:", best_mtry, "\n")
# Train the Random Forest model with the best mtry value
set.seed(123)
best_rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = best_mtry, importance = TRUE)
# Make predictions on the test data
test_predictions <- predict(best_rf_model, test_data)
# Create a data frame with actual and predicted values
results <- data.frame(
Actual = test_data$GDP,
Predicted = test_predictions
)
# Plot the actual values vs. predicted values
actual_vs_predicted_plot <- ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Actual vs. Predicted Values",
x = "Actual GDP",
y = "Predicted GDP") +
theme_minimal()
# Display the plot
print(actual_vs_predicted_plot)
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define a range of mtry values to test
mtry_values <- seq(1, ncol(train_data) - 1, by = 1)
# Store the OOB error rates for each mtry value
oob_errors <- data.frame(
mtry = integer(),
OOBError = numeric()
)
# Loop over different mtry values
for (mtry in mtry_values) {
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = mtry, importance = TRUE)
# Store the mtry value and corresponding OOB error
oob_errors <- rbind(oob_errors, data.frame(mtry = mtry, OOBError = tail(rf_model$mse, 1)))
}
# Plot the OOB error rate for different mtry values
oob_error_plot <- ggplot(oob_errors, aes(x = mtry, y = OOBError)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "OOB Error Rate vs. Number of Variables Tried at Each Split (mtry)",
x = "Number of Variables (mtry)",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Print all the OOB values
print(oob_errors)
# Identify the mtry value with the lowest OOB error rate
best_mtry <- oob_errors$mtry[which.min(oob_errors$OOBError)]
cat("Best mtry value:", best_mtry, "\n")
# Train the Random Forest model with the best mtry value
set.seed(123)
best_rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = best_mtry, importance = TRUE)
# Make predictions on the test data
test_predictions <- predict(best_rf_model, test_data)
# Create a data frame with actual and predicted values
results <- data.frame(
Actual = test_data$GDP,
Predicted = test_predictions
)
# Plot the actual values vs. predicted values
actual_vs_predicted_plot <- ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Actual vs. Predicted Values",
x = "Actual GDP",
y = "Predicted GDP") +
theme_minimal()
# Display the plot
print(actual_vs_predicted_plot)
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
library(rpart)
library(rpart.plot)
install.packages("rpart.plot")
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define a range of mtry values to test
mtry_values <- seq(1, ncol(train_data) - 1, by = 1)
# Store the OOB error rates for each mtry value
oob_errors <- data.frame(
mtry = integer(),
OOBError = numeric()
)
# Loop over different mtry values
for (mtry in mtry_values) {
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = mtry, importance = TRUE)
# Store the mtry value and corresponding OOB error
oob_errors <- rbind(oob_errors, data.frame(mtry = mtry, OOBError = tail(rf_model$mse, 1)))
}
# Plot the OOB error rate for different mtry values
oob_error_plot <- ggplot(oob_errors, aes(x = mtry, y = OOBError)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "OOB Error Rate vs. Number of Variables Tried at Each Split (mtry)",
x = "Number of Variables (mtry)",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Print all the OOB values
print(oob_errors)
# Identify the mtry value with the lowest OOB error rate
best_mtry <- oob_errors$mtry[which.min(oob_errors$OOBError)]
cat("Best mtry value:", best_mtry, "\n")
# Train the Random Forest model with the best mtry value
set.seed(123)
best_rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = best_mtry, importance = TRUE)
# Make predictions on the test data
test_predictions <- predict(best_rf_model, test_data)
# Create a data frame with actual and predicted values
results <- data.frame(
Actual = test_data$GDP,
Predicted = test_predictions
)
# Plot the actual values vs. predicted values
actual_vs_predicted_plot <- ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Actual vs. Predicted Values",
x = "Actual GDP",
y = "Predicted GDP") +
theme_minimal()
# Display the plot
print(actual_vs_predicted_plot)
# Extract a single tree from the Random Forest model
single_tree <- getTree(best_rf_model, k = 1, labelVar = TRUE)
# Convert the tree to an rpart object for plotting
single_tree_rpart <- as.rpart(single_tree)
# Load necessary libraries
library(dplyr)
library(readr)
library(writexl)
library(tidyr)
library(randomForest)
library(ggplot2)
library(rpart)
library(rpart.plot)
# Load the dataset
data <- read.csv("master_data_with_gdp_india.csv")
# Split the data into training and testing sets
set.seed(123)  # For reproducibility
train_indices <- sample(seq_len(nrow(data)), size = 0.8 * nrow(data))
train_data <- data[train_indices, ]
test_data <- data[-train_indices, ]
# Define a range of mtry values to test
mtry_values <- seq(1, ncol(train_data) - 1, by = 1)
# Store the OOB error rates for each mtry value
oob_errors <- data.frame(
mtry = integer(),
OOBError = numeric()
)
# Loop over different mtry values
for (mtry in mtry_values) {
# Train the Random Forest model
set.seed(123)
rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = mtry, importance = TRUE)
# Store the mtry value and corresponding OOB error
oob_errors <- rbind(oob_errors, data.frame(mtry = mtry, OOBError = tail(rf_model$mse, 1)))
}
# Plot the OOB error rate for different mtry values
oob_error_plot <- ggplot(oob_errors, aes(x = mtry, y = OOBError)) +
geom_line(color = "blue") +
geom_point(color = "red") +
labs(title = "OOB Error Rate vs. Number of Variables Tried at Each Split (mtry)",
x = "Number of Variables (mtry)",
y = "OOB Error Rate (MSE)") +
theme_minimal()
# Display the plot
print(oob_error_plot)
# Print all the OOB values
print(oob_errors)
# Identify the mtry value with the lowest OOB error rate
best_mtry <- oob_errors$mtry[which.min(oob_errors$OOBError)]
cat("Best mtry value:", best_mtry, "\n")
# Train the Random Forest model with the best mtry value
set.seed(123)
best_rf_model <- randomForest(GDP ~ ., data = train_data, ntree = 500, mtry = best_mtry, importance = TRUE)
# Make predictions on the test data
test_predictions <- predict(best_rf_model, test_data)
# Create a data frame with actual and predicted values
results <- data.frame(
Actual = test_data$GDP,
Predicted = test_predictions
)
# Plot the actual values vs. predicted values
actual_vs_predicted_plot <- ggplot(results, aes(x = Actual, y = Predicted)) +
geom_point(alpha = 0.6) +
geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
labs(title = "Actual vs. Predicted Values",
x = "Actual GDP",
y = "Predicted GDP") +
theme_minimal()
# Display the plot
print(actual_vs_predicted_plot)
# Train a single decision tree using rpart
decision_tree <- rpart(GDP ~ ., data = train_data)
# Plot the decision tree
rpart.plot(decision_tree, main = "Decision Tree for GDP Prediction", type = 3, extra = 1)
